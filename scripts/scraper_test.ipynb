{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versnellingsplan Kennisbank URL Extractor Test\n",
    "\n",
    "This notebook tests URL extraction from the Versnellingsplan Kennisbank website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import nest_asyncio\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops (required for Jupyter)\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path to import project modules\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('scraper-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Crawl4AI and Configure"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "try:\n    import crawl4ai\n    logger.info(\"Crawl4AI imported successfully.\")\nexcept ImportError as e:\n    logger.error(f\"Failed to import Crawl4AI: {e}\")\n    logger.info(\"Installing Crawl4AI...\")\n    !pip install crawl4ai\n    import crawl4ai"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "async def extract_kennisbank_urls(url=\"https://www.versnellingsplan.nl/kennisbank/\"):\n    \"\"\"Extract URLs from the Versnellingsplan Kennisbank page.\n    \n    Args:\n        url (str): The URL of the Kennisbank page\n        \n    Returns:\n        list: List of dictionaries with 'title' and 'url' keys\n    \"\"\"\n    try:\n        logger.info(f\"Extracting URLs from {url}\")\n        \n        # Create AsyncWebCrawler instance\n        async with crawl4ai.AsyncWebCrawler() as crawler:\n            page = await crawler.arun(url=url)\n            \n            # Wait for the content links to appear (10s timeout)\n            await page.wait_for('a.elementor-post__thumbnail__link', timeout=10000)\n            \n            # Extract the links\n            links = page.soup.select('a.elementor-post__thumbnail__link')\n            \n            results = []\n            for link in links:\n                try:\n                    url = link.get('href')\n                    if not url:\n                        continue\n                        \n                    # Make sure URL is absolute\n                    if not url.startswith('http'):\n                        url = f\"{url.rstrip('/')}/{url.lstrip('/')}\"\n                    \n                    # Try to get title from the link\n                    title_elem = link.select_one('.elementor-post__title')\n                    title = title_elem.text.strip() if title_elem else link.get('title', url)\n                    \n                    results.append({\"title\": title, \"url\": url})\n                except Exception as e:\n                    logger.warning(f\"Error processing link: {e}\")\n            \n            logger.info(f\"Extracted {len(results)} URLs\")\n            return results\n    \n    except Exception as e:\n        logger.error(f\"Error extracting URLs: {e}\")\n        return []"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format URLs as Markdown Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_as_markdown_links(url_data):\n",
    "    \"\"\"Format URL data as markdown links.\n",
    "    \n",
    "    Args:\n",
    "        url_data (list): List of dictionaries with 'title' and 'url' keys\n",
    "        \n",
    "    Returns:\n",
    "        str: Markdown formatted list of links\n",
    "    \"\"\"\n",
    "    if not url_data:\n",
    "        return \"No URLs found.\"\n",
    "    \n",
    "    markdown_links = []\n",
    "    for item in url_data:\n",
    "        title = item.get(\"title\", \"Untitled\")\n",
    "        url = item.get(\"url\", \"\")\n",
    "        if url:\n",
    "            markdown_links.append(f\"- [{title}]({url})\")\n",
    "    \n",
    "    return \"\\n\".join(markdown_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run URL Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    # Extract URLs from Kennisbank\n",
    "    url_data = await extract_kennisbank_urls()\n",
    "    \n",
    "    # Format and display as markdown\n",
    "    markdown_text = format_as_markdown_links(url_data)\n",
    "    \n",
    "    # Display as plain text\n",
    "    print(\"Extracted URLs (Plain Text):\")\n",
    "    print(markdown_text)\n",
    "    \n",
    "    # Display as rendered markdown\n",
    "    return Markdown(\"### Extracted URLs (Rendered):\\n\" + markdown_text)\n",
    "\n",
    "# Run the main function\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Analysis of a Single URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def analyze_single_url(url_index=0):\n",
    "    \"\"\"Analyze a single URL from the extracted list.\n",
    "    \n",
    "    Args:\n",
    "        url_index (int): Index of the URL in the extracted list\n",
    "        \n",
    "    Returns:\n",
    "        dict: Analysis results\n",
    "    \"\"\"\n",
    "    # Extract URLs\n",
    "    url_data = await extract_kennisbank_urls()\n",
    "    \n",
    "    if not url_data or url_index >= len(url_data):\n",
    "        return {\"error\": \"URL not found\"}\n",
    "    \n",
    "    # Get the URL to analyze\n",
    "    target_url = url_data[url_index].get(\"url\")\n",
    "    target_title = url_data[url_index].get(\"title\")\n",
    "    \n",
    "    logger.info(f\"Analyzing URL: {target_title} - {target_url}\")\n",
    "    \n",
    "    try:\n",
    "        # Create crawler instance\n",
    "        crawler = AsyncHTMLCrawler()\n",
    "        \n",
    "        # Fetch the page\n",
    "        response = await crawler.fetch(target_url)\n",
    "        \n",
    "        if not response.ok:\n",
    "            return {\"error\": f\"Failed to fetch {target_url}: {response.status}\"}\n",
    "        \n",
    "        # Extract metadata\n",
    "        title_selector = XPathSelector(\"//h1\", attr=\"text\")\n",
    "        content_selector = XPathSelector(\"//article//div[contains(@class, 'content')]\", attr=\"html\")\n",
    "        \n",
    "        page_title = await title_selector.extract(response)\n",
    "        content = await content_selector.extract(response)\n",
    "        \n",
    "        # Extract any PDF links\n",
    "        pdf_selector = XPathSelector(\"//a[contains(@href, '.pdf')]\", attr={\"url\": \"href\", \"text\": \"text\"})\n",
    "        pdf_links = await pdf_selector.extract_all(response)\n",
    "        \n",
    "        return {\n",
    "            \"title\": page_title,\n",
    "            \"url\": target_url,\n",
    "            \"content_preview\": content[:500] + \"...\" if content else \"No content found\",\n",
    "            \"pdf_links\": pdf_links\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error analyzing URL: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Run the analysis for the first URL (change index as needed)\n",
    "analysis = await analyze_single_url(0)\n",
    "analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}