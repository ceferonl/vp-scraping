{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Scraper Demo\"\n",
        "format: html\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "# Scraper Demo: Crawl4AI + Polars\n",
        "\n",
        "This document demonstrates the scraping pipeline for a few sample URLs using the project modules.\n",
        "\n",
        "## 1. Extract Content URLs\n"
      ],
      "id": "a11ba35b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add parent directory to path so we can import from src\n",
        "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))))\n",
        "from src.url_extractor import extract_content_urls\n",
        "\n",
        "urls = extract_content_urls(\"https://www.versnellingsplan.nl/kennisbank/\")\n",
        "print(f\"Found {len(urls)} URLs:\")\n",
        "for url in urls[:3]:\n",
        "    print(url)"
      ],
      "id": "1d25c101",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Scrape Content for a Sample URL\n"
      ],
      "id": "4cd6f9ed"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from src.content_scraper import scrape_content\n",
        "\n",
        "sample_url = urls[0] if urls else \"https://www.versnellingsplan.nl/kennisbank/some-article\"\n",
        "content = scrape_content(sample_url)\n",
        "print(content)"
      ],
      "id": "09c0f16d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Download PDFs for the Sample URL\n"
      ],
      "id": "7f4a1f49"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(\"../output/demo/pdfs\", exist_ok=True)\n",
        "\n",
        "from src.pdf_downloader import download_pdfs\n",
        "pdfs = download_pdfs(content.get('pdf_links', []), \"../output/demo/pdfs\")\n",
        "print(pdfs)"
      ],
      "id": "3396b864",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Generate JSON Output\n"
      ],
      "id": "9f0690a6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(\"../output/demo\", exist_ok=True)\n",
        "\n",
        "from src.json_generator import generate_json\n",
        "json_path = generate_json(content, pdfs, \"../output/demo\")\n",
        "print(f\"JSON saved at: {json_path}\")"
      ],
      "id": "139a7788",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "This demo shows the end-to-end workflow for scraping, PDF downloading, and JSON output using Crawl4AI and project modules."
      ],
      "id": "808b8e03"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/corneeldenhartogh/repositories/vp-scraping/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}